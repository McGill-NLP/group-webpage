---
title: The Effect of Domain and Diacritics in Yoruba–English Neural Machine Translation
venue: Machine Translation Summit
names: David Ifeoluwa Adelani, Dana Ruiter, Jesujoba Oluwadara Alabi, Damilola Adebonojo,
  Adesina Ayeni, Mofetoluwa Adeyemi, Ayodele Awokoya, C. España-Bonet
tags:
- Machine Translation Summit
link: https://arxiv.org/abs/2103.08647
author: David Adelani
categories: Publications
layout: paper

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Massively multilingual machine translation (MT) has shown impressive capabilities and including zero and few-shot translation between low-resource language pairs. However and these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due to lack of standardized evaluation datasets. In this paper and we present MENYO-20k and the first multi-domain parallel corpus with a especially curated orthography for Yoruba–English with standardized train-test splits for benchmarking. We provide several neural MT benchmarks and compare them to the performance of popular pre-trained (massively multilingual) MT models both for the heterogeneous test set and its subdomains. Since these pre-trained models use huge amounts of data with uncertain quality and we also analyze the effect of diacritics and a major characteristic of Yoruba and in the training data. We investigate how and when this training condition affects the final quality of a translation and its understandability. Our models outperform massively multilingual models such as Google (+8.7 BLEU) and Facebook M2M (+9.1) when translating to Yoruba and setting a high quality benchmark for future research.
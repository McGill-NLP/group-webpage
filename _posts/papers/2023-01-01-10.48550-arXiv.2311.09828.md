---
title: 'AfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced African
  Languages'
venue: arXiv.org
names: Jiayi Wang, David Ifeoluwa Adelani, Sweta Agrawal, Ricardo Rei, Eleftheria
  Briakou, Marine Carpuat, Marek Masiak, Xuanli He, Sofia Bourhim, Andiswa Bukula,
  Muhidin A. Mohamed, Temitayo Olatoye, Hamam Mokayede, Christine Mwase, Wangui Kimotho,
  Foutse Yuehgoh, Anuoluwapo Aremu, Jessica Ojo, Shamsuddeen Hassan Muhammad, Salomey
  Osei, Abdul-Hakeem Omotayo, Chiamaka Chukwuneke, Perez Ogayo, Oumaima Hourrane,
  Salma El Anigri, Lolwethu Ndolela, Thabiso Mangwana, Shafie Abdi Mohamed, Ayinde
  Hassan, Oluwabusayo Olufunke Awoyomi, Lama Alkhaled, S. Al-Azzawi, Naome A. Etori,
  Millicent Ochieng, Clemencia Siro, Samuel Njoroge, Eric Muchiri, Wangari Kimotho,
  Lyse Naomi Wamba Momo, D. Abolade, Simbiat Ajao, Tosin P. Adewumi, Iyanuoluwa Shode,
  Ricky Macharm, R. Iro, S. S. Abdullahi, Stephen E. Moore, Bernard Opoku, Zainab
  Akinjobi, Abeeb Afolabi, Nnaemeka Obiefuna, Onyekachi Raphael Ogbu, Sam Brian, V.
  Otiende, C. Mbonu, Sakayo Toadoum Sari, Pontus Stenetorp
tags:
- arXiv.org
link: https://doi.org/10.48550/arXiv.2311.09828
author: David Adelani
categories: Publications
layout: paper

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Despite the progress we have recorded in scaling multilingual machine translation (MT) models and evaluation data to several under-resourced African languages, it is difficult to measure accurately the progress we have made on these languages because evaluation is often performed on n -gram matching metrics like BLEU that often have worse correlation with human judgments. Embedding-based metrics such as COMET correlate better; however, lack of evaluation data with human ratings for under-resourced languages, complexity of annotation guidelines like Multidimensional Quality Metrics (MQM), and limited language coverage of multilingual encoders have hampered their applicability to African languages. In this paper, we address these challenges by creating high-quality human evaluation data with a simplified MQM guideline for error-span annotation and direct assessment (DA) scoring for 13 typologi-cally diverse African languages. Furthermore, we develop A FRI COMETâ€”a COMET evaluation metric for African languages by leveraging DA training data from high-resource languages and African-centric multilingual encoder (AfroXLM-Roberta) to create the state-of-the-art evaluation metric for African languages MT with respect to Spearman-rank correlation with human judgments ( +0 . 406 ).
---
title: 'MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition'
venue: Conference on Empirical Methods in Natural Language Processing
names: David Ifeoluwa Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael
  Beukman, Chester Palen-Michel, Constantine Lignos, Jesujoba Oluwadara Alabi, Shamsuddeen
  Hassan Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa Bukula, Rooweither
  Mabuya, Bonaventure F. P. Dossou, Blessing K. Sibanda, Happy Buzaaba, Jonathan Mukiibi,
  Godson Kalipe, Derguene Mbaye, Amelia Taylor, F. Kabore, Chris C. Emezue, Anuoluwapo
  Aremu, Perez Ogayo, C. Gitau, Edwin Munkoh-Buabeng, V. M. Koagne, A. Tapo, Tebogo
  Macucwa, Vukosi Marivate, Elvis Mboning, T. Gwadabe, Tosin P. Adewumi, Orevaoghene
  Ahia, J. Nakatumba‚ÄêNabende, Neo L. Mokono, Ignatius M Ezeani, C. Chukwuneke, Mofetoluwa
  Adeyemi, Gilles Hacheme, Idris Abdulmumin, Odunayo Ogundepo, Oreen Yousuf, Tatiana
  Moteu Ngoli, D. Klakow
tags:
- Conference on Empirical Methods in Natural Language Processing
link: https://arxiv.org/abs/2210.12391
author: David Adelani
categories: Publications
layout: paper

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and recently proposed methods like cross-lingual transfer will be effective. In this paper, we aim to move towards solutions for these challenges, focusing on the task of named entity recognition (NER). We present the creation of the largest to-date human-annotated NER dataset for 20 African languages. We study the behaviour of state-of-the-art cross-lingual transfer methods in an Africa-centric setting, empirically demonstrating that the choice of source transfer language significantly affects performance. While much previous work defaults to using English as the source language, our results show that choosing the best transfer language improves zero-shot F1 scores by an average of 14% over 20 languages as compared to using English.
---
title: Few-Shot Pidgin Text Adaptation via Contrastive Fine-Tuning
venue: International Conference on Computational Linguistics
names: Ernie Chang, Jesujoba Oluwadara Alabi, David Ifeoluwa Adelani, Vera Demberg
tags:
- International Conference on Computational Linguistics
link: https://www.semanticscholar.org/paper/b1f69004b3c7d409d55fea21441b1e3a4e8940dd
author: David Adelani
categories: Publications
layout: paper

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

The surging demand for multilingual dialogue systems often requires a costly labeling process for each language addition. For low resource languages, human annotators are continuously tasked with the adaptation of resource-rich language utterances for each new domain. However, this prohibitive and impractical process can often be a bottleneck for low resource languages that are still without proper translation systems nor parallel corpus. In particular, it is difficult to obtain task-specific low resource language annotations for the English-derived creoles (e.g. Nigerian and Cameroonian Pidgin). To address this issue, we utilize the pretrained language models i.e. BART which has shown great potential in language generation/understanding â€“ we propose to finetune the BART model to generate utterances in Pidgin by leveraging the proximity of the source and target languages, and utilizing positive and negative examples in constrastive training objectives. We collected and released the first parallel Pidgin-English conversation corpus in two dialogue domains and showed that this simple and effective technique is suffice to yield impressive results for English-to-Pidgin generation, which are two closely-related languages.
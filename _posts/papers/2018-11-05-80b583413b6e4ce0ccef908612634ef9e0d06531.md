---
title: On the Evaluation of Common-Sense Reasoning in Natural Language Understanding
venue: arXiv.org
names: P. Trichelair, Ali Emami, J. Cheung, A. Trischler, Kaheer Suleman, Fernando
  Diaz
tags:
- arXiv.org
link: https://www.semanticscholar.org/paper/80b583413b6e4ce0ccef908612634ef9e0d06531
author: Jackie Cheung
categories: Publications

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

The NLP and ML communities have long been interested in developing models capable of common-sense reasoning, and recent works have significantly improved the state of the art on benchmarks like the Winograd Schema Challenge (WSC). Despite these advances, the complexity of tasks designed to test common-sense reasoning remains under-analyzed. In this paper, we make a case study of the Winograd Schema Challenge and, based on two new measures of instance-level complexity, design a protocol that both clarifies and qualifies the results of previous work. Our protocol accounts for the WSC's limited size and variable instance difficulty, properties common to other common-sense benchmarks. Accounting for these properties when assessing model results may prevent unjustified conclusions.
---
title: "Post-hoc Interpretability for Neural NLP: A Survey" # Add official title
author: Andreas Madsen # Add name to show profile in sidebar
categories: Publications # Used to list all posts about publications in /publications/
names: "Andreas Madsen, Siva Reddy, Sarath Chandar" # names of all authors
link: https://arxiv.org/abs/2108.04840 # link to paper
twitter: https://twitter.com/andreas_madsen/status/1425794989741727748  # link to twitter thread (optional)
thumbnail: /assets/images/papers/2021-nlp_interpretability_survey.svg  # link to a thumbnail (optional)
venue: ArXiv pre-print  # venue and year of the paper
tags: pre-print # tag of the paper (exclude year, use shorthand)
---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.

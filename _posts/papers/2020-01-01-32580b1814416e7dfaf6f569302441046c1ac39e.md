---
title: 'Optimizing Deeper Transformers on Small Datasets: An Application on Text-to-SQL
  Semantic Parsing'
venue: arXiv
names: Peng Xu, Wei Yang, Wenjie Zi, Keyi Tang, Chengyang Huang, J. Cheung, Yanshuai
  Cao
tags:
- arXiv
link: https://www.semanticscholar.org/paper/32580b1814416e7dfaf6f569302441046c1ac39e
author: Jackie Cheung
categories: Publications

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Due to the common belief that training deep transformers from scratch requires large datasets, people usually only use shallow and simple additional layers on top of pre-trained models during ﬁne-tuning on small datasets. We provide evidence that this does not al-ways need to be the case: with proper initialization and training techniques, the bene-ﬁts of very deep transformers are shown to carry over to hard structural prediction tasks, even using small datasets. In particular, we successfully train 48 layers of transformers for a semantic parsing task. These com-prise 24 ﬁne-tuned transformer layers from pre-trained RoBERTa and 24 relation-aware transformer layers trained from scratch. With fewer training steps and no task-speciﬁc pretraining, we obtain the state of the art performance on the challenging cross-domain Text-to-SQL semantic parsing benchmark Spider. We achieve this by deriving a novel D ata dependent T ransformer Fix ed- up date initialization scheme (DT-Fixup), inspired by the prior T-Fixup work (Huang et al., 2020). Further error analysis demonstrates that increasing the depth of the transformer model can help improve generalization on the cases requiring reasoning and structural understanding 1 .
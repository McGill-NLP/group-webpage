---
title: Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization
venue: arXiv.org
names: Mengyao Cao, Yue Dong, J. Cheung
tags:
- arXiv.org
link: https://www.semanticscholar.org/paper/86f22c32e6ea59659732b33aba1a786125e6f585
author: Jackie Cheung
categories: Publications

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

State-of-the-art abstractive summarization systems often generate hallucinations ; i.e., content that is not directly inferable from the source text. Despite being assumed incorrect, many of the hallucinated contents are consis-tent with world knowledge (factual hallucinations). Including these factual hallucinations into a summary can be beneﬁcial in provid-ing additional background information. In this work, we propose a novel detection approach that separates factual from non-factual hallucinations of entities. Our method is based on an entity’s prior and posterior probabilities according to pre-trained and ﬁnetuned masked language models, respectively. Empirical results suggest that our method vastly outperforms three strong baselines in both accuracy and F1 scores and has a strong correlation with human judgements on factuality classiﬁcation tasks. Furthermore, our approach can provide insight into whether a particular hallucination is caused by the summarizer’s pre-training or ﬁne-tuning step. 1
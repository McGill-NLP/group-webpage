---
title: D IALOGUE P IDGIN T EXT A DAPTATION VIA C ON - TRASTIVE F INE -T UNING
venue: ''
names: Ernie Chang, Jesujoba Oluwadara Alabi, David Ifeoluwa Adelani, Vera Demberg
tags:
- ''
link: https://www.semanticscholar.org/paper/2ef570ac8db5c7e5192334f31675cc2fd7b6622a
author: David Adelani
categories: Publications
layout: paper

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

The surging demand for multilingual dialogue systems often requires a costly labeling process for each language addition. For low resource languages, human annotators are continuously tasked with the adaptation of resource-rich language utterances for each new domain. However, this prohibitive and impractical process can often be a bottleneck for low resource languages that are still without proper translation systems nor parallel corpus. In particular, it is difficult to obtain task-specific low resource language annotations for the English-derived creoles (e.g. Nigerian and Cameroonian Pidgin). To address this issue, we utilize the pretrained language models i.e. BART which has shown great potential in language generation/understanding â€“ we propose to finetune the BART model to generate utterances in Pidgin by leveraging the proximity of the source and target languages, and utilizing positive and negative examples in contrastive training objectives. We collected and released the first parallel PidginEnglish conversation corpus in two dialogue domains and showed that this simple and effective technique is sufficient to yield impressive results for English-to-Pidgin generation, which are two closely-related languages.
---
title: "Intern Presentations"
venue: McGill + Mila
names:  Steven Koniaev, Nathan Zeweniuk
author: Steven Koniaev, Nathan Zeweniuk
tags:
- NLP RG
categories:
    - Reading-Group
    - Fall-2024
layout: archive
classes:
    - wide
    - no-sidebar
---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

The [NLP Reading Group]({% link _pages/reading-group.md %}) is thrilled to showcase the work of our McGill NLP interns who will each be giving 10 minute talks (5 minutes of QA) to the group. The speakers will be [Steven Koniaev](https://steven-koniaev.vercel.app/) and [Nathan Zeweniuk](https://mila.quebec/en/directory/nathan-zeweniuk).

More information about each talk below.


# Steven Koniaev

## Talk Description

The vast volume of text generated in recent years has resulted in a growing demand for effective summarization methods. Consequently, various approaches have been introduced for this purpose, achieving commendable evaluation scores. However, despite the success of these models in summarizing most texts, they still exhibit lower scores for certain cases.  his observation leads to the hypothesis that some texts pose greater challenges for summarization. In this project, our aim is to develop a model that takes a document as input and predicts its summarization difficulty accurately. By utilizing this model and analyze its results, we can understand in which cases the text becomes more challenging to summarize, and apply this to certain NLP tasks such as Multidocument Summarization. 

## Bio

Steven is a current undergraduate researcher at McGill University under Prof. Jackie Cheung working alongside Ori Ernst.

# Nathan Zeweniuk

## Talk Description

Summarization has become an important part of making large amounts of information more accessible to readers. Summarization methods are commonly described as either 'extractive', where salient text is copied directly from the source, or 'abstractive', where the source information can be reworded or altered. Previous attempts to describe a level of abstractiveness have been based on simple lexical measures, such as novel n-grams, which can fail to capture important semantic differences between sources and summaries. In this project, we analyze the levels of abstractiveness in human written summaries based on differences of information between the source and summary. Using this analysis, we are able to show that summarization systems struggle to generate reference summary content with the same level of abstractiveness.

## Bio

Nathan recently graduated from McGill University where he worked on research with Ori Ernst under Prof. Jackie Cheung. He is currently pursuing a master's degree at the University of Alberta.

# Logistics

Date: September 27th<br>
Time: 11:30AM <br>
Location: Auditorium 2 or via Zoom (See email)

